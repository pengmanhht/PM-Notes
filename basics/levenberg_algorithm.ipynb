{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://daily-tech.hatenablog.com/entry/2017/03/21/063518\n",
    "\n",
    "https://medium.com/@hirok4/python-implementation-of-levenberg-marquardt-algorithm-8ff8abdec0f5\n",
    "\n",
    "“The Levenberg-Marquardt Algorithm: Implementation and Theory,” Numerical Analysis, ed. G. A. Watson, Lecture Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenberg method is a combination of gradient descent and Newton method.\n",
    "$$x_{n+1} = x_n - (\\Delta^2 f(x_n) + \\lambda I)^{-1} \\Delta f(x_n)$$\n",
    "when $\\lambda$->0, the LM method approaches the NR method and when $\\lambda$->$\\infin$, it approaches the GD mehtod with small step sizes.\n",
    "\n",
    "When the gain $\\lambda$ is sufficiently large, even if the Hessian matrix is not positive definite, the matrix $H + \\lambda I$ can be positive definite and gaurantee a reduction in the function's value\n",
    "\n",
    "If in each step of uptake, the cost function f(x) goes down (which implies that the curvature is helping), we accept the step and we reduce $\\lambda$ (usually by a factor of 10) to reduce the influence of gradient descent. On the other hand, if the cost function goes up, we retract the step and increase $\\lambda$ by a factor of 10 or some significant factor.\n",
    "\n",
    "In terms of convergence speed: GD < LM < NR\n",
    "\n",
    "In terms of convergence stability: GD > LM > NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp \n",
    "from jax import grad, jit, vmap, jacfwd, hessian, random\n",
    "from jax.tree_util import tree_map, tree_flatten\n",
    "from jax.experimental.ode import odeint \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton-Raphson Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.507141   0.48427495]\n",
      "[2.4896717  0.81633145]\n",
      "[2.4896717  0.81633145]\n",
      "[2.4896717  0.81633145]\n",
      "[2.4896717  0.81633145]\n",
      "[2.4896717  0.81633145]\n",
      "[2.4896717  0.81633145]\n",
      "[2.4896717  0.81633145]\n",
      "[2.4896717  0.81633145]\n",
      "[2.4896717  0.81633145]\n"
     ]
    }
   ],
   "source": [
    "def forward(params, x):\n",
    "    a, b = params[0], params[1]\n",
    "    return b*jnp.exp(a * x)\n",
    "\n",
    "# toy data\n",
    "x = jnp.linspace(1, 2, 20)\n",
    "params_true = jnp.array([\n",
    "    2.5,\n",
    "    0.8\n",
    "])\n",
    "y = forward(params_true, x) + random.normal(key=random.PRNGKey(123), shape=(len(x),)) * 0.5\n",
    "# plt.plot(x, y)\n",
    "\n",
    "def cost_fn(params, x, y):\n",
    "    pred = forward(params, x)\n",
    "    err  = pred - y\n",
    "    return jnp.mean(jnp.square(err))\n",
    "\n",
    "# print(cost_fn(params_true, x, y))\n",
    "\n",
    "# def newton_raphson(cost_fn, params, x, y):\n",
    "#     jacob = jacfwd(cost_fn, argnums=0)\n",
    "#     hess  = hessian(cost_fn,argnums=0)\n",
    "#     return tree_map(lambda p, j, h: p - jnp.invert(h) @ jnp.transpose(j), params, jacob, hess)\n",
    "\n",
    "params = jnp.array([\n",
    "    2.4,  # poor convergence when the starting values are far from the final estimates\n",
    "    0.5\n",
    "])\n",
    "\n",
    "for i in range(1000):\n",
    "    jacob = jacfwd(cost_fn, argnums=0)\n",
    "    jac = jacob(params, x, y)\n",
    "\n",
    "    hess = hessian(cost_fn, argnums=0)\n",
    "    hes = hess(params, x, y)\n",
    "\n",
    "    params -=  jnp.linalg.inv(hes) @ jac.T\n",
    "    if i % 100 == 0:\n",
    "        print(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenberg-Marquardt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:        0 | Loss: 14650948416700416.0000 | Est: [9.946395  1.0005617]\n",
      "Iter:      500 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n",
      "Iter:     1000 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n",
      "Iter:     1500 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n",
      "Iter:     2000 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n",
      "Iter:     2500 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n",
      "Iter:     3000 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n",
      "Iter:     3500 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n",
      "Iter:     4000 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n",
      "Iter:     4500 | Loss:   0.2475 | Est: [2.489671  0.8163325]\n"
     ]
    }
   ],
   "source": [
    "def forward(params, x):\n",
    "    a, b= params[0], params[1]\n",
    "    return b*jnp.exp(a * x)\n",
    "\n",
    "# toy data\n",
    "x = jnp.linspace(1, 2, 20)\n",
    "params_true = jnp.array([\n",
    "    2.5,\n",
    "    0.8\n",
    "])\n",
    "y = forward(params_true, x) + random.normal(key=random.PRNGKey(123), shape=(len(x),)) * 0.5\n",
    "# plt.plot(x, y)\n",
    "\n",
    "def cost_fn(params, x, y):\n",
    "    pred = forward(params, x)\n",
    "    err  = pred - y\n",
    "    return jnp.mean(jnp.square(err))\n",
    "\n",
    "\n",
    "params = jnp.array([\n",
    "    10.2,\n",
    "    1.\n",
    "])\n",
    "n_iter = 5000\n",
    "\n",
    "for i in range(n_iter):\n",
    "    jacob = jacfwd(cost_fn, argnums=0)\n",
    "    jac = jacob(params, x, y)\n",
    "\n",
    "    hess = hessian(cost_fn, argnums=0)\n",
    "    hes = hess(params, x, y)\n",
    "\n",
    "    # add a small constat [identity / diagonal matrix] to Hes, such that the Hes remains positive definite\n",
    "    params -=  jnp.linalg.inv(hes + 1e3*jnp.identity(hes.shape[0])) @ jac.T \n",
    "    \n",
    "    # use diagonal such that when lambda -> 0, algorithm appcoaches the Gauss-Newton\n",
    "    # when lambda -> inf, algorithm approaches the Gradient Descent\n",
    "    # params -=  jnp.linalg.inv(hes + 1/(i+1)*jnp.diag(hes)) @ jac.T \n",
    "    if i % (n_iter/10) == 0:\n",
    "        print(f\"Iter: {i:8} | Loss: {cost_fn(params, x, y):8.4f} | Est: {params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numpyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
